{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q-Learning: Aprendendo as Melhores Ações por Tentativa e Erro**\n",
        "\n",
        "Imagine que você está tentando ensinar um cachorro a sentar. Você não o ensina com uma lista de instruções. Em vez disso, você espera que ele tente algo (talvez ele sente por acidente), e quando ele faz o que você quer, você o recompensa (com um petisco ou carinho). Com o tempo, ele associa a ação \"sentar\" com a recompensa.\n",
        "\n",
        "O Q-Learning funciona de forma semelhante. Um agente (o \"cachorro\") explora um ambiente (o \"mundo\"), toma ações, recebe recompensas (ou punições) e observa o novo estado do ambiente. Seu objetivo é aprender uma política (um conjunto de regras de comportamento) que o guie a tomar as melhores ações em cada estado para maximizar a recompensa total a longo prazo.\n",
        "\n",
        "A \"inteligência\" do Q-Learning reside em uma tabela chamada Tabela Q (Q-Table).\n",
        "\n",
        "A Tabela Q (Q-Table)\n",
        "\n",
        "* É uma tabela que armazena os \"valores Q\" para cada par (estado, ação).\n",
        "* Um valor Q (Q-value) representa a qualidade (Q) de tomar uma determinada ação em um determinado estado e, a partir daí, seguir a melhor política possível.\n",
        "* Quanto maior o valor Q para um (estado, ação), melhor é essa ação naquele estado."
      ],
      "metadata": {
        "id": "KQbYkhC4rYaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDhx3KmTrmKt",
        "outputId": "b7af0f08-d59f-41a5-df8f-e0e7594ae8ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym # Usando gymnasium que é o sucessor do gym\n",
        "import random\n",
        "from IPython.display import clear_output # Para limpar a saída no Colab/Jupyter\n",
        "import time # Para adicionar um pequeno delay na renderização\n",
        "\n",
        "print(\"Iniciando a demonstração didática de Q-Learning...\\n\")\n",
        "\n",
        "# --- 1. Configuração do Ambiente e Parâmetros ---\n",
        "print(\"PASSO 1: Configurando o ambiente Frozen Lake e os parâmetros do Q-Learning.\")\n",
        "\n",
        "# Criando o ambiente Frozen Lake (is_slippery=False para simplificar e garantir a convergência rápida)\n",
        "# Com is_slippery=True, o ambiente é estocástico (ações podem não levar onde esperado), o que é mais complexo.\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "env.reset() # Resetar o ambiente para o estado inicial\n",
        "\n",
        "# Parâmetros do Q-Learning\n",
        "learning_rate = 0.9      # Alfa (α): Quão rápido o agente atualiza os valores Q\n",
        "discount_factor = 0.95   # Gamma (γ): Importância das recompensas futuras\n",
        "epsilon = 1.0            # Epsilon (ε): Probabilidade de explorar (começa alto)\n",
        "epsilon_decay_rate = 0.001 # Taxa de decaimento do epsilon por episódio\n",
        "min_epsilon = 0.01       # Epsilon mínimo para garantir alguma exploração\n",
        "n_episodes = 2000        # Número total de episódios de treinamento\n",
        "\n",
        "# Inicialização da Tabela Q\n",
        "# A tabela Q terá o formato (num_estados, num_acoes)\n",
        "# Preenchemos com zeros no início, pois o agente não sabe nada.\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "print(f\"  - Número de Estados: {env.observation_space.n}\") # 16 estados (4x4)\n",
        "print(f\"  - Número de Ações: {env.action_space.n}\")     # 4 ações (cima, baixo, esquerda, direita)\n",
        "print(f\"  - Tabela Q inicial (primeiras 5 linhas):\\n{q_table[:5]}\\n\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "# --- 2. Treinamento do Agente (Q-Learning Loop) ---\n",
        "print(\"PASSO 2: Iniciando o treinamento do agente usando Q-Learning.\")\n",
        "print(\"  -> O agente explorará o lago congelado, aprendendo os valores Q por tentativa e erro.\")\n",
        "\n",
        "rewards_per_episode = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state, info = env.reset() # Resetar o ambiente a cada novo episódio (voltar ao início)\n",
        "    done = False              # Flag para indicar se o episódio terminou (chegou ao objetivo ou buraco)\n",
        "    trunc = False             # Flag para indicar se o episódio foi truncado (limite de passos)\n",
        "    current_episode_reward = 0\n",
        "\n",
        "    while not done and not trunc:\n",
        "        # 1. Escolha da Ação (Epsilon-Greedy Strategy)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Exploração: Escolhe uma ação aleatória\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :]) # Explotação: Escolhe a ação com o maior Q-value\n",
        "\n",
        "        # 2. Tomada da Ação e Observação do Novo Estado e Recompensa\n",
        "        new_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # 3. Atualização do Valor Q (Fórmula do Q-Learning)\n",
        "        # Q(s, a) <- Q(s, a) + α * [r + γ * max(Q(s', a')) - Q(s, a)]\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * \\\n",
        "                                 (reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action])\n",
        "\n",
        "        state = new_state # Move para o novo estado\n",
        "        current_episode_reward += reward\n",
        "\n",
        "    # Decaimento do Epsilon (reduz a exploração com o tempo)\n",
        "    epsilon = max(min_epsilon, epsilon - epsilon_decay_rate)\n",
        "\n",
        "    rewards_per_episode.append(current_episode_reward)\n",
        "\n",
        "    if (episode + 1) % (n_episodes // 10) == 0:\n",
        "        print(f\"  - Episódio {episode + 1}/{n_episodes} - Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"\\n  Treinamento concluído!\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "# --- 3. Análise dos Resultados do Treinamento ---\n",
        "print(\"PASSO 3: Analisando o desempenho do treinamento.\")\n",
        "\n",
        "# Média de recompensas por blocos de episódios (para suavizar a curva)\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_per_episode), n_episodes/1000)\n",
        "count = 1000\n",
        "print(\"  Média de recompensas por bloco de 1000 episódios:\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(f\"    - Episódios {count-999}-{count}: {sum(r/1000)}\")\n",
        "    count += 1000\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(pd.Series(rewards_per_episode).rolling(window=100).mean()) # Média móvel de 100 episódios\n",
        "plt.title('Recompensa Média por Episódio (Média Móvel de 100 Episódios)', fontsize=14, weight='bold')\n",
        "plt.xlabel('Episódio')\n",
        "plt.ylabel('Recompensa Média')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n  A Tabela Q final (representando a política aprendida, primeiras 5 linhas):\\n\")\n",
        "print(q_table[:5])\n",
        "print(\"\\n  Os valores na tabela Q indicam a 'qualidade' de cada ação em cada estado.\")\n",
        "print(\"  Valores mais altos significam ações melhores.\\n\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# --- 4. Avaliação do Agente Treinado (Teste) ---\n",
        "print(\"PASSO 4: Avaliando o agente treinado (sem exploração).\")\n",
        "print(\"  -> O agente agora usará a política aprendida (apenas explotação).\")\n",
        "\n",
        "n_eval_episodes = 10\n",
        "total_rewards_eval = 0\n",
        "\n",
        "for episode in range(n_eval_episodes):\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    trunc = False\n",
        "    print(f\"\\n--- Avaliação - Episódio {episode + 1} ---\")\n",
        "    time.sleep(0.5) # Pequeno delay para visualização\n",
        "\n",
        "    while not done and not trunc:\n",
        "        clear_output(wait=True) # Limpa a tela no Colab/Jupyter\n",
        "        print(f\"--- Avaliação - Episódio {episode + 1} ---\")\n",
        "        print(env.render()) # Mostra o ambiente\n",
        "        time.sleep(0.3)\n",
        "\n",
        "        action = np.argmax(q_table[state, :]) # Escolhe a melhor ação aprendida\n",
        "        new_state, reward, done, trunc, info = env.step(action)\n",
        "        state = new_state\n",
        "        total_rewards_eval += reward\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f\"--- Avaliação - Episódio {episode + 1} ---\")\n",
        "    print(env.render()) # Renderiza o estado final\n",
        "    if reward > 0:\n",
        "        print(\"🎉 Agente alcançou o objetivo! 🎉\")\n",
        "    else:\n",
        "        print(\"😔 Agente caiu em um buraco ou esgotou os passos. 😔\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(f\"\\n\\n  Média de recompensas na avaliação: {total_rewards_eval / n_eval_episodes:.2f}\")\n",
        "print(\"  Se o valor for próximo de 1.0, o agente está alcançando o objetivo consistentemente.\")\n",
        "print(\"  Se o agente cair em um buraco, a recompensa é 0 (ou negativa dependendo do ambiente).\")\n",
        "print(\"\\n--- Demonstração de Q-Learning Concluída! ---\")\n",
        "\n",
        "# Fechar o ambiente\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UpJKhSTirtel",
        "outputId": "fc4614d9-ff4f-48b3-e930-e9690dede367"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Avaliação - Episódio 10 ---\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "🎉 Agente alcançou o objetivo! 🎉\n",
            "\n",
            "\n",
            "  Média de recompensas na avaliação: 1.00\n",
            "  Se o valor for próximo de 1.0, o agente está alcançando o objetivo consistentemente.\n",
            "  Se o agente cair em um buraco, a recompensa é 0 (ou negativa dependendo do ambiente).\n",
            "\n",
            "--- Demonstração de Q-Learning Concluída! ---\n"
          ]
        }
      ]
    }
  ]
}