{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q-Learning: Aprendendo as Melhores AÃ§Ãµes por Tentativa e Erro**\n",
        "\n",
        "Imagine que vocÃª estÃ¡ tentando ensinar um cachorro a sentar. VocÃª nÃ£o o ensina com uma lista de instruÃ§Ãµes. Em vez disso, vocÃª espera que ele tente algo (talvez ele sente por acidente), e quando ele faz o que vocÃª quer, vocÃª o recompensa (com um petisco ou carinho). Com o tempo, ele associa a aÃ§Ã£o \"sentar\" com a recompensa.\n",
        "\n",
        "O Q-Learning funciona de forma semelhante. Um agente (o \"cachorro\") explora um ambiente (o \"mundo\"), toma aÃ§Ãµes, recebe recompensas (ou puniÃ§Ãµes) e observa o novo estado do ambiente. Seu objetivo Ã© aprender uma polÃ­tica (um conjunto de regras de comportamento) que o guie a tomar as melhores aÃ§Ãµes em cada estado para maximizar a recompensa total a longo prazo.\n",
        "\n",
        "A \"inteligÃªncia\" do Q-Learning reside em uma tabela chamada Tabela Q (Q-Table).\n",
        "\n",
        "A Tabela Q (Q-Table)\n",
        "\n",
        "* Ã‰ uma tabela que armazena os \"valores Q\" para cada par (estado, aÃ§Ã£o).\n",
        "* Um valor Q (Q-value) representa a qualidade (Q) de tomar uma determinada aÃ§Ã£o em um determinado estado e, a partir daÃ­, seguir a melhor polÃ­tica possÃ­vel.\n",
        "* Quanto maior o valor Q para um (estado, aÃ§Ã£o), melhor Ã© essa aÃ§Ã£o naquele estado."
      ],
      "metadata": {
        "id": "KQbYkhC4rYaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDhx3KmTrmKt",
        "outputId": "b7af0f08-d59f-41a5-df8f-e0e7594ae8ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym # Usando gymnasium que Ã© o sucessor do gym\n",
        "import random\n",
        "from IPython.display import clear_output # Para limpar a saÃ­da no Colab/Jupyter\n",
        "import time # Para adicionar um pequeno delay na renderizaÃ§Ã£o\n",
        "\n",
        "print(\"Iniciando a demonstraÃ§Ã£o didÃ¡tica de Q-Learning...\\n\")\n",
        "\n",
        "# --- 1. ConfiguraÃ§Ã£o do Ambiente e ParÃ¢metros ---\n",
        "print(\"PASSO 1: Configurando o ambiente Frozen Lake e os parÃ¢metros do Q-Learning.\")\n",
        "\n",
        "# Criando o ambiente Frozen Lake (is_slippery=False para simplificar e garantir a convergÃªncia rÃ¡pida)\n",
        "# Com is_slippery=True, o ambiente Ã© estocÃ¡stico (aÃ§Ãµes podem nÃ£o levar onde esperado), o que Ã© mais complexo.\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n",
        "env.reset() # Resetar o ambiente para o estado inicial\n",
        "\n",
        "# ParÃ¢metros do Q-Learning\n",
        "learning_rate = 0.9      # Alfa (Î±): QuÃ£o rÃ¡pido o agente atualiza os valores Q\n",
        "discount_factor = 0.95   # Gamma (Î³): ImportÃ¢ncia das recompensas futuras\n",
        "epsilon = 1.0            # Epsilon (Îµ): Probabilidade de explorar (comeÃ§a alto)\n",
        "epsilon_decay_rate = 0.001 # Taxa de decaimento do epsilon por episÃ³dio\n",
        "min_epsilon = 0.01       # Epsilon mÃ­nimo para garantir alguma exploraÃ§Ã£o\n",
        "n_episodes = 2000        # NÃºmero total de episÃ³dios de treinamento\n",
        "\n",
        "# InicializaÃ§Ã£o da Tabela Q\n",
        "# A tabela Q terÃ¡ o formato (num_estados, num_acoes)\n",
        "# Preenchemos com zeros no inÃ­cio, pois o agente nÃ£o sabe nada.\n",
        "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "print(f\"  - NÃºmero de Estados: {env.observation_space.n}\") # 16 estados (4x4)\n",
        "print(f\"  - NÃºmero de AÃ§Ãµes: {env.action_space.n}\")     # 4 aÃ§Ãµes (cima, baixo, esquerda, direita)\n",
        "print(f\"  - Tabela Q inicial (primeiras 5 linhas):\\n{q_table[:5]}\\n\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "# --- 2. Treinamento do Agente (Q-Learning Loop) ---\n",
        "print(\"PASSO 2: Iniciando o treinamento do agente usando Q-Learning.\")\n",
        "print(\"  -> O agente explorarÃ¡ o lago congelado, aprendendo os valores Q por tentativa e erro.\")\n",
        "\n",
        "rewards_per_episode = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state, info = env.reset() # Resetar o ambiente a cada novo episÃ³dio (voltar ao inÃ­cio)\n",
        "    done = False              # Flag para indicar se o episÃ³dio terminou (chegou ao objetivo ou buraco)\n",
        "    trunc = False             # Flag para indicar se o episÃ³dio foi truncado (limite de passos)\n",
        "    current_episode_reward = 0\n",
        "\n",
        "    while not done and not trunc:\n",
        "        # 1. Escolha da AÃ§Ã£o (Epsilon-Greedy Strategy)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # ExploraÃ§Ã£o: Escolhe uma aÃ§Ã£o aleatÃ³ria\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :]) # ExplotaÃ§Ã£o: Escolhe a aÃ§Ã£o com o maior Q-value\n",
        "\n",
        "        # 2. Tomada da AÃ§Ã£o e ObservaÃ§Ã£o do Novo Estado e Recompensa\n",
        "        new_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # 3. AtualizaÃ§Ã£o do Valor Q (FÃ³rmula do Q-Learning)\n",
        "        # Q(s, a) <- Q(s, a) + Î± * [r + Î³ * max(Q(s', a')) - Q(s, a)]\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * \\\n",
        "                                 (reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action])\n",
        "\n",
        "        state = new_state # Move para o novo estado\n",
        "        current_episode_reward += reward\n",
        "\n",
        "    # Decaimento do Epsilon (reduz a exploraÃ§Ã£o com o tempo)\n",
        "    epsilon = max(min_epsilon, epsilon - epsilon_decay_rate)\n",
        "\n",
        "    rewards_per_episode.append(current_episode_reward)\n",
        "\n",
        "    if (episode + 1) % (n_episodes // 10) == 0:\n",
        "        print(f\"  - EpisÃ³dio {episode + 1}/{n_episodes} - Epsilon: {epsilon:.2f}\")\n",
        "\n",
        "print(\"\\n  Treinamento concluÃ­do!\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "# --- 3. AnÃ¡lise dos Resultados do Treinamento ---\n",
        "print(\"PASSO 3: Analisando o desempenho do treinamento.\")\n",
        "\n",
        "# MÃ©dia de recompensas por blocos de episÃ³dios (para suavizar a curva)\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_per_episode), n_episodes/1000)\n",
        "count = 1000\n",
        "print(\"  MÃ©dia de recompensas por bloco de 1000 episÃ³dios:\")\n",
        "for r in rewards_per_thousand_episodes:\n",
        "    print(f\"    - EpisÃ³dios {count-999}-{count}: {sum(r/1000)}\")\n",
        "    count += 1000\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(pd.Series(rewards_per_episode).rolling(window=100).mean()) # MÃ©dia mÃ³vel de 100 episÃ³dios\n",
        "plt.title('Recompensa MÃ©dia por EpisÃ³dio (MÃ©dia MÃ³vel de 100 EpisÃ³dios)', fontsize=14, weight='bold')\n",
        "plt.xlabel('EpisÃ³dio')\n",
        "plt.ylabel('Recompensa MÃ©dia')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n  A Tabela Q final (representando a polÃ­tica aprendida, primeiras 5 linhas):\\n\")\n",
        "print(q_table[:5])\n",
        "print(\"\\n  Os valores na tabela Q indicam a 'qualidade' de cada aÃ§Ã£o em cada estado.\")\n",
        "print(\"  Valores mais altos significam aÃ§Ãµes melhores.\\n\")\n",
        "print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "# --- 4. AvaliaÃ§Ã£o do Agente Treinado (Teste) ---\n",
        "print(\"PASSO 4: Avaliando o agente treinado (sem exploraÃ§Ã£o).\")\n",
        "print(\"  -> O agente agora usarÃ¡ a polÃ­tica aprendida (apenas explotaÃ§Ã£o).\")\n",
        "\n",
        "n_eval_episodes = 10\n",
        "total_rewards_eval = 0\n",
        "\n",
        "for episode in range(n_eval_episodes):\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    trunc = False\n",
        "    print(f\"\\n--- AvaliaÃ§Ã£o - EpisÃ³dio {episode + 1} ---\")\n",
        "    time.sleep(0.5) # Pequeno delay para visualizaÃ§Ã£o\n",
        "\n",
        "    while not done and not trunc:\n",
        "        clear_output(wait=True) # Limpa a tela no Colab/Jupyter\n",
        "        print(f\"--- AvaliaÃ§Ã£o - EpisÃ³dio {episode + 1} ---\")\n",
        "        print(env.render()) # Mostra o ambiente\n",
        "        time.sleep(0.3)\n",
        "\n",
        "        action = np.argmax(q_table[state, :]) # Escolhe a melhor aÃ§Ã£o aprendida\n",
        "        new_state, reward, done, trunc, info = env.step(action)\n",
        "        state = new_state\n",
        "        total_rewards_eval += reward\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f\"--- AvaliaÃ§Ã£o - EpisÃ³dio {episode + 1} ---\")\n",
        "    print(env.render()) # Renderiza o estado final\n",
        "    if reward > 0:\n",
        "        print(\"ğŸ‰ Agente alcanÃ§ou o objetivo! ğŸ‰\")\n",
        "    else:\n",
        "        print(\"ğŸ˜” Agente caiu em um buraco ou esgotou os passos. ğŸ˜”\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(f\"\\n\\n  MÃ©dia de recompensas na avaliaÃ§Ã£o: {total_rewards_eval / n_eval_episodes:.2f}\")\n",
        "print(\"  Se o valor for prÃ³ximo de 1.0, o agente estÃ¡ alcanÃ§ando o objetivo consistentemente.\")\n",
        "print(\"  Se o agente cair em um buraco, a recompensa Ã© 0 (ou negativa dependendo do ambiente).\")\n",
        "print(\"\\n--- DemonstraÃ§Ã£o de Q-Learning ConcluÃ­da! ---\")\n",
        "\n",
        "# Fechar o ambiente\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UpJKhSTirtel",
        "outputId": "fc4614d9-ff4f-48b3-e930-e9690dede367"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- AvaliaÃ§Ã£o - EpisÃ³dio 10 ---\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "ğŸ‰ Agente alcanÃ§ou o objetivo! ğŸ‰\n",
            "\n",
            "\n",
            "  MÃ©dia de recompensas na avaliaÃ§Ã£o: 1.00\n",
            "  Se o valor for prÃ³ximo de 1.0, o agente estÃ¡ alcanÃ§ando o objetivo consistentemente.\n",
            "  Se o agente cair em um buraco, a recompensa Ã© 0 (ou negativa dependendo do ambiente).\n",
            "\n",
            "--- DemonstraÃ§Ã£o de Q-Learning ConcluÃ­da! ---\n"
          ]
        }
      ]
    }
  ]
}